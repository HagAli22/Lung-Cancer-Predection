{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "85ead628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # type: ignore\n",
    "\n",
    "\n",
    "dataset_path = \"cancer_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "f4e7c4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 612 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1\n",
    "    )\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    f'{dataset_path}/train',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "4688139c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 173 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Validition\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_set = val_datagen.flow_from_directory(\n",
    "    f'{dataset_path}/valid',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "366ba0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN|\n",
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "ae8fc157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matrix Store\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "# Step1 - Convolution\n",
    "cnn.add(tf.keras.layers.Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=3,\n",
    "    activation='relu',\n",
    "    input_shape=[64, 64, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "82fd3cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2 - Pooling\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "c0c39d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn.add(tf.keras.layers.Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "b7f5a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd Convolutional layer\n",
    "cnn.add(tf.keras.layers.Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=3,\n",
    "    activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "dcca3c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn.add(tf.keras.layers.Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "13de9818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd Convolutional layer\n",
    "cnn.add(tf.keras.layers.Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=3,\n",
    "    activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "1630cc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3 - Fallenting\n",
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "8318f27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step4 - Full Connection\n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "9b01314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step5 - Output Layer\n",
    "cnn.add(tf.keras.layers.Dense(units=4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "e611939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the CNN\n",
    "cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "ec5bb282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# افترض إن عندك أسماء الكلاسات:\n",
    "class_names = list(training_set.class_indices.keys())\n",
    "\n",
    "# خد كل الليبلز من generator\n",
    "labels = training_set.classes  # أرقام الكلاسات\n",
    "\n",
    "# احسب class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(labels),\n",
    "    y=labels\n",
    ")\n",
    "\n",
    "# حولهم لقاموس\n",
    "class_weights = dict(enumerate(class_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "d937bec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matrix Store\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 203ms/step - accuracy: 0.2838 - loss: 1.3595 - val_accuracy: 0.3642 - val_loss: 1.3105\n",
      "Epoch 2/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 177ms/step - accuracy: 0.3977 - loss: 1.2541 - val_accuracy: 0.3295 - val_loss: 1.2852\n",
      "Epoch 3/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 209ms/step - accuracy: 0.4630 - loss: 1.1495 - val_accuracy: 0.3815 - val_loss: 1.1741\n",
      "Epoch 4/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 187ms/step - accuracy: 0.4942 - loss: 1.0800 - val_accuracy: 0.5145 - val_loss: 1.0566\n",
      "Epoch 5/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 228ms/step - accuracy: 0.5862 - loss: 1.0109 - val_accuracy: 0.4855 - val_loss: 1.0620\n",
      "Epoch 6/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 175ms/step - accuracy: 0.6531 - loss: 0.7994 - val_accuracy: 0.5954 - val_loss: 0.8527\n",
      "Epoch 7/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.6437 - loss: 0.8158 - val_accuracy: 0.5491 - val_loss: 1.1377\n",
      "Epoch 8/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - accuracy: 0.6017 - loss: 0.8372 - val_accuracy: 0.5549 - val_loss: 1.0504\n",
      "Epoch 9/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.6995 - loss: 0.7268 - val_accuracy: 0.5029 - val_loss: 0.9272\n",
      "Epoch 10/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 162ms/step - accuracy: 0.6883 - loss: 0.7437 - val_accuracy: 0.5723 - val_loss: 1.1018\n",
      "Epoch 11/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 249ms/step - accuracy: 0.7216 - loss: 0.6709 - val_accuracy: 0.5723 - val_loss: 0.9247\n",
      "Epoch 12/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 195ms/step - accuracy: 0.7074 - loss: 0.6190 - val_accuracy: 0.6474 - val_loss: 0.8219\n",
      "Epoch 13/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 201ms/step - accuracy: 0.7110 - loss: 0.6109 - val_accuracy: 0.6069 - val_loss: 0.8380\n",
      "Epoch 14/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 185ms/step - accuracy: 0.7698 - loss: 0.5259 - val_accuracy: 0.6069 - val_loss: 0.8888\n",
      "Epoch 15/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 191ms/step - accuracy: 0.7625 - loss: 0.5590 - val_accuracy: 0.5665 - val_loss: 1.0142\n",
      "Epoch 16/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 185ms/step - accuracy: 0.7166 - loss: 0.6115 - val_accuracy: 0.6127 - val_loss: 0.8089\n",
      "Epoch 17/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 210ms/step - accuracy: 0.8024 - loss: 0.5096 - val_accuracy: 0.6590 - val_loss: 0.6480\n",
      "Epoch 18/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 171ms/step - accuracy: 0.7465 - loss: 0.5110 - val_accuracy: 0.5954 - val_loss: 0.8933\n",
      "Epoch 19/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 149ms/step - accuracy: 0.8282 - loss: 0.4367 - val_accuracy: 0.6301 - val_loss: 1.0076\n",
      "Epoch 20/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 214ms/step - accuracy: 0.8273 - loss: 0.4704 - val_accuracy: 0.6590 - val_loss: 0.7777\n",
      "Epoch 21/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 191ms/step - accuracy: 0.8000 - loss: 0.4825 - val_accuracy: 0.6474 - val_loss: 0.8566\n",
      "Epoch 22/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 206ms/step - accuracy: 0.8377 - loss: 0.4470 - val_accuracy: 0.6243 - val_loss: 0.8307\n",
      "Epoch 23/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 186ms/step - accuracy: 0.7880 - loss: 0.5330 - val_accuracy: 0.6243 - val_loss: 0.9552\n",
      "Epoch 24/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 191ms/step - accuracy: 0.8180 - loss: 0.4581 - val_accuracy: 0.6532 - val_loss: 0.7704\n",
      "Epoch 25/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 183ms/step - accuracy: 0.8016 - loss: 0.4424 - val_accuracy: 0.6358 - val_loss: 0.9183\n",
      "Epoch 26/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 189ms/step - accuracy: 0.8423 - loss: 0.3964 - val_accuracy: 0.7110 - val_loss: 0.6213\n",
      "Epoch 27/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 194ms/step - accuracy: 0.8616 - loss: 0.3747 - val_accuracy: 0.6705 - val_loss: 0.7551\n",
      "Epoch 28/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 184ms/step - accuracy: 0.8467 - loss: 0.3529 - val_accuracy: 0.6763 - val_loss: 0.9017\n",
      "Epoch 29/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 182ms/step - accuracy: 0.8411 - loss: 0.3958 - val_accuracy: 0.6705 - val_loss: 0.9058\n",
      "Epoch 30/30\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 167ms/step - accuracy: 0.8574 - loss: 0.3430 - val_accuracy: 0.6763 - val_loss: 0.7940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x24010a75f90>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the CNN on the Training set\n",
    "# and evaluation it on the Test set\n",
    "cnn.fit(x = training_set, validation_data=val_set, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "87bfd04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cancer_dataset/test\\adenocarcinoma\\000149 (7).png\n",
      "WARNING:tensorflow:6 out of the last 19 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000240072CA2A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 19 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000240072CA2A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
      "[[0. 0. 1. 0.]]\n",
      "{'adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib': 0, 'large.cell.carcinoma_left.hilum_T2_N2_M0_IIIa': 1, 'normal': 2, 'squamous.cell.carcinoma_left.hilum_T1_N2_M0_IIIa': 3}\n",
      "Adenocarcinoma\n"
     ]
    }
   ],
   "source": [
    "# Making a Single prediction\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import os\n",
    "import random\n",
    "\n",
    "test_folders = ['adenocarcinoma', 'large.cell.carcinoma', 'normal', 'squamous.cell.carcinoma']\n",
    "random_folder = random.choice(test_folders)\n",
    "folder_path = os.path.join(f'{dataset_path}/test', random_folder)\n",
    "images = os.listdir(folder_path)\n",
    "random_image = random.choice(images)\n",
    "image_path = os.path.join(folder_path, random_image)\n",
    "\n",
    "print(image_path)\n",
    "\n",
    "test_image = image.load_img(image_path, target_size=(64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "result = cnn.predict(test_image)\n",
    "\n",
    "print(result)\n",
    "print(training_set.class_indices)\n",
    "\n",
    "if result[0][0] == 0:\n",
    "    prediction = 'Adenocarcinoma'\n",
    "elif result[0][0] == 1:\n",
    "    prediction = 'Large Cell Carcinoma'\n",
    "elif result[0][0] == 2:\n",
    "    prediction = 'Normal'\n",
    "elif result[0][0] == 3:\n",
    "    prediction = 'Squanmous Cell Carcinoma'\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd9d05d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
